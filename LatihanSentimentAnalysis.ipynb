{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/apple-sentiment.csv', 'r') as nodecsv: # Open the file                       \n",
    "    csvreader = csv.reader(nodecsv) # Read the csv  \n",
    "    # Retrieve the data (using Python list comprhension and list slicing to remove the header row)\n",
    "    csv = [n for n in csvreader][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes \n",
    "        \\S*\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = []\n",
    "regex_str.append(emoticons_str)\n",
    "regex_str.append(r'<[^>]+>')# HTML tags\n",
    "regex_str.append(r'(?:@[\\w_]+)')# @-mentions\n",
    "regex_str.append(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\") # hash-tags\n",
    "regex_str.append(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+') # URLs\n",
    "regex_str.append(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)') # numbers\n",
    "regex_str.append(r\"(?:[a-z][a-z'\\-_]+[a-z])\") # words with - and '\n",
    "regex_str.append(r'(?:[\\w_]+)') # other words\n",
    "regex_str.append(r'(?:\\S)') # anything else\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '…','•']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n",
      "!\n",
      "\"\n",
      "#\n",
      "$\n",
      "%\n",
      "&\n",
      "'\n",
      "(\n",
      ")\n",
      "*\n",
      "+\n",
      ",\n",
      "-\n",
      ".\n",
      "/\n",
      ":\n",
      ";\n",
      "<\n",
      "=\n",
      ">\n",
      "?\n",
      "@\n",
      "[\n",
      "\\\n",
      "]\n",
      "^\n",
      "_\n",
      "`\n",
      "{\n",
      "|\n",
      "}\n",
      "~\n",
      "rt\n",
      "via\n",
      "…\n",
      "•\n"
     ]
    }
   ],
   "source": [
    "for l in stop:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens = tokens_re.findall(s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(token,regex):\n",
    "    terms_all = [emoji.demojize(term) for term in token if term.lower() not in stop and not regex.match(term)]\n",
    "    return terms_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for c in csv:\n",
    "    tokens.append(tokenize(c[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Global', 'warming', 'report', 'urges', 'governments', 'to', 'act', '|', 'BRUSSELS', ',', 'Belgium', '(', 'AP', ')', '-', 'The', 'world', 'faces', 'increased', 'hunger', 'and', '.', '.', '[', 'link', ']'], ['Fighting', 'poverty', 'and', 'global', 'warming', 'in', 'Africa', '[', 'link', ']'], ['Carbon', 'offsets', ':', 'How', 'a', 'Vatican', 'forest', 'failed', 'to', 'reduce', 'global', 'warming', '[', 'link', ']'], ['Carbon', 'offsets', ':', 'How', 'a', 'Vatican', 'forest', 'failed', 'to', 'reduce', 'global', 'warming', '[', 'link', ']'], ['URUGUAY', ':', 'Tools', 'Needed', 'for', 'Those', 'Most', 'Vulnerable', 'to', 'Climate', 'Change', '[', 'link', ']']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('climate', 3422), ('change', 3301), ('global', 3121), ('warming', 3057), ('link', 978), ('new', 334), ('bill', 247), ('snow', 239), ('news', 223), ('science', 197), ('report', 179), ('energy', 179), ('us', 173), ('world', 158), ('à_', 146), ('u', 144), ('dc', 141), ('day', 138), ('could', 136), ('earth', 135), ('obama', 133), ('scientists', 131), ('green', 130), ('al', 120), ('gore', 117), ('conference', 110), ('fight', 109), ('weather', 108), ('un', 108), ('may', 107), ('senate', 106), ('think', 106), ('one', 104), ('legislation', 103), ('says', 101), ('get', 101), ('say', 101), ('time', 101), ('carbon', 96), ('believe', 96), ('com', 95), ('washington', 94), ('help', 94), ('great', 93), ('agency', 91), ('immigration', 90), ('people', 89), ('law', 88), ('winter', 87), ('worse', 86), ('good', 83), ('april', 82), ('action', 81), ('due', 80), ('cold', 80), ('like', 79), ('stop', 78), ('still', 78), ('take', 77), ('study', 76), ('make', 73), ('p', 73), ('makes', 72), ('air', 71), ('debate', 71), ('real', 70), ('video', 70), ('volcano', 70), ('c', 69), ('climate-change', 68), ('clinical', 68), ('trials', 68), ('environmental', 68), ('much', 67), ('federal', 67), ('effects', 66), ('live', 66), ('india', 65), ('allergies', 65), ('talks', 65), ('ice', 64), ('blog', 63), ('another', 63), ('panel', 63), ('blizzard', 63), ('water', 62), ('health', 62), ('part', 62), ('save', 61), (\"people's\", 60), ('know', 60), ('impacts', 60), ('bolivia', 59), ('must', 58), ('causes', 58), ('reuters', 58), ('state', 58), ('really', 57), ('thing', 56), ('money', 56)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "exclude_str = []\n",
    "exclude_str.append(emoticons_str)\n",
    "exclude_str.append(r'<[^>]+>')# HTML tags\n",
    "exclude_str.append(r'(?:@[\\w_]+)')# @-mentions\n",
    "exclude_str.append(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\") # hash-tags\n",
    "exclude_str.append(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+') # URLs\n",
    "exclude_str.append(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)') # numbers\n",
    "\n",
    "exclude_re = re.compile(r'('+'|'.join(exclude_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "all_word=[]\n",
    "from collections import Counter\n",
    "count_all = Counter()\n",
    "for token in tokens:\n",
    "    cleanToken = cleanTweet(token,exclude_re)\n",
    "    terms_all = [term.lower() for term in cleanToken]\n",
    "    count_all.update(terms_all)\n",
    "    for w in terms_all:\n",
    "        all_word.append(w)\n",
    "print(count_all.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Photo: Love IOS 8 @apple @iphone #Love #IOS8 #NoCrop #iphone #young #phone #screen #capturescream #boy... http://t.co/fIDj9FxHDV\"\n",
    "token=tokenize(text)\n",
    "cleanToken=cleanTweet(token,exclude_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photo: Love IOS 8 @apple @iphone #Love #IOS8 #NoCrop #iphone #young #phone #screen #capturescream #boy... http://t.co/fIDj9FxHDV\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Photo', ':', 'Love', 'IOS', '8', '@apple', '@iphone', '#Love', '#IOS8', '#NoCrop', '#iphone', '#young', '#phone', '#screen', '#capturescream', '#boy', '.', '.', '.', 'http://t.co/fIDj9FxHDV']\n"
     ]
    }
   ],
   "source": [
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Photo', 'Love', 'IOS', '@apple', '@iphone']\n"
     ]
    }
   ],
   "source": [
    "print(cleanToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos = nltk.pos_tag(all_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  j = adjective, r = adverb, n=noun and v = verb\n",
    "allowed_word_types = [\"N\",\"J\",\"V\"]\n",
    "all_adj=[w[0] for w in pos if w[1][0] in allowed_word_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = nltk.FreqDist(all_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(allwords.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global', 'warming', 'report', 'urges', 'governments', 'act', 'brussels', 'belgium', 'ap', 'world', 'faces', 'increased', 'hunger', 'link', 'fighting', 'poverty', 'africa', 'carbon', 'offsets', 'vatican', 'forest', 'failed', 'reduce', 'uruguay', 'tools', 'needed', 'vulnerable', 'climate', 'change', 'ocean', 'saltiness', 'shows', 'intensifying', 'water', 'cycle', 'evidence', 'message', 'deniers', 'clueless', 'kennedy', 'flies', 'pri', 'physicists', 'violates', 'physics', 'reveal', 'uab', 'clarendon', 'shirt', 'laughs']\n"
     ]
    }
   ],
   "source": [
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document = [(w[0],w[1]) for w in csv]\n",
    "document=[]\n",
    "for w in csv:\n",
    "    categ=\"\"\n",
    "    if w[1] in ['Yes','Y','5']:\n",
    "        categ=\"pos\"\n",
    "        document.append((w[0],categ))\n",
    "    if w[1] in ['No','N','1']:\n",
    "        categ=\"neg\"\n",
    "        document.append((w[0],categ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'global': False, 'warming': True, 'report': True, 'urges': True, 'governments': True, 'act': False, 'brussels': False, 'belgium': False, 'ap': False, 'world': True, 'faces': True, 'increased': True, 'hunger': True, 'link': True, 'fighting': False, 'poverty': False, 'africa': False, 'carbon': False, 'offsets': False, 'vatican': False, 'forest': False, 'failed': False, 'reduce': False, 'uruguay': False, 'tools': False, 'needed': False, 'vulnerable': False, 'climate': False, 'change': False, 'ocean': False, 'saltiness': False, 'shows': False, 'intensifying': False, 'water': False, 'cycle': False, 'evidence': False, 'message': False, 'deniers': False, 'doubters': False, 'look': False, 'migratory': False, 'birds': False, 'new': False, 'strategy': False, 'stay': False, 'southern': False, 'competing': False, 'limpopo': False, 'bring': False, 'higher': False, 'temperatures': False, 'southe': False, 'impact': False, 'wheat': False, 'rice': False, 'production': False, 'india': False, 'ludhiana': False, 'apr': False, 'scarcity': False, 'serious': False, 'solve': False, 'thing': False, 'blog': False, 'preliminary': False, 'analysis': False, 'suggests': False, 'natural': False, 'gas': False, 'contribute': False, 'ecotone': False, 'perspective': False, 'blamed': False, 'coastal': False, 'whale': False}, 'pos')]\n"
     ]
    }
   ],
   "source": [
    "print(featuresets[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyusun data training dan testing \n",
    "trainsize = round(len(featuresets)*0.7)\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "training_set = featuresets[:trainsize]\n",
    "testing_set = featuresets[trainsize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p[1]=='neg' for p in training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 76.87450670876085\n",
      "Most Informative Features\n",
      "                 hearing = True              neg : pos    =     16.1 : 1.0\n",
      "                 control = True              neg : pos    =     16.1 : 1.0\n",
      "                imminent = True              neg : pos    =     14.2 : 1.0\n",
      "                   snows = True              neg : pos    =     14.2 : 1.0\n",
      "                disaster = True              neg : pos    =     12.3 : 1.0\n",
      "                  shovel = True              neg : pos    =     12.3 : 1.0\n",
      "                happened = True              neg : pos    =     12.3 : 1.0\n",
      "               alarmists = True              neg : pos    =     12.3 : 1.0\n",
      "                      al = True              neg : pos    =     11.9 : 1.0\n",
      "                    hoax = True              neg : pos    =     11.8 : 1.0\n",
      "                    gore = True              neg : pos    =     10.8 : 1.0\n",
      "                   stuff = True              neg : pos    =     10.4 : 1.0\n",
      "               shoveling = True              neg : pos    =     10.4 : 1.0\n",
      "              scientific = True              neg : pos    =      9.7 : 1.0\n",
      "                   exist = True              neg : pos    =      9.7 : 1.0\n",
      "                     gop = True              neg : pos    =      9.3 : 1.0\n",
      "                   fight = True              pos : neg    =      8.6 : 1.0\n",
      "                     lol = True              neg : pos    =      8.5 : 1.0\n",
      "                election = True              neg : pos    =      8.5 : 1.0\n",
      "                    idea = True              neg : pos    =      8.5 : 1.0\n",
      "                    want = True              neg : pos    =      8.5 : 1.0\n",
      "                 calling = True              neg : pos    =      8.5 : 1.0\n",
      "                  errors = True              neg : pos    =      8.5 : 1.0\n",
      "                 current = True              neg : pos    =      8.5 : 1.0\n",
      "                 suspend = True              neg : pos    =      8.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8728260869565218\n",
      "Precision: 0.8373305526590198\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import (precision, recall)\n",
    "import collections\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(testing_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print('Recall:', recall(refsets['pos'], testsets['pos']))\n",
    "print('Precision:', precision(refsets['pos'], testsets['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0))>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2 = nltk.classify.SklearnClassifier(LinearSVC())\n",
    "classifier2.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 81.29439621152328\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier2, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SklearnClassifier(LogisticRegression(solver='lbfgs')).train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 81.05761641673243\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier3, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Senate CAP & TAX *** Bill to be unveiled April 26 *** Global Warming Swindle (Video's too)|Our work is cut out .. http://oohja.com/xb3rj - neg - neg\n",
      "Algae for controlling global warming - Current News & Events ... http://bit.ly/ax6lEp - pos - pos\n",
      "Addressing Global Warming: Retrain Coal Workers for Green Jobs ... http://bit.ly/cW1KTV - pos - pos\n",
      "Forests Are Growing Faster,Climate Change Appears to Be Driving Accelerated Growth (Science Daily) http://bit.ly/d1EqlN @InvasiveNotes - pos - pos\n",
      "Obama's base cares about their religion of global warming and Marxism. The myth of healthcare. The myth of peace without having to be strong - neg - neg\n",
      "Global warming may become global cooling this century http://bit.ly/cU8D4F - pos - pos\n",
      "Global Warming: Ozone Hole Healing http://bit.ly/aXpdcl - pos - pos\n",
      "My daughter says Global Warming Is NOT Cool!  http://twitgoo.com/ok5ya - pos - pos\n",
      "Earth's polar ice sheets vulnerable to even moderate global warming; New Orleans, much of southern F... http://cli.gs/yEg8T - pos - pos\n"
     ]
    }
   ],
   "source": [
    "for (rev, category) in document[trainsize:]:\n",
    "    result=classifier2.classify(find_features(rev))\n",
    "    print(rev,\"-\",category,\"-\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
